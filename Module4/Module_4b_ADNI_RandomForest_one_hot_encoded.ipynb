{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pleunipennings/CSC508_ML_Biomedicine_Class/blob/main/Module4/Module_4b_ADNI_RandomForest_one_hot_encoded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8DIh94Mkokf"
      },
      "source": [
        "#Working with the ADNI PatData data and creating a Random Forest model for predicting Alzheimer's status. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eypv_1yPXe2X"
      },
      "source": [
        "# 1) Opening the file location and loading libraries\n",
        "Importing all necessary libraries to create our models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F48UupL7Lt5S"
      },
      "source": [
        "# Data cleaning and wrangling packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer \n",
        "\n",
        "# Machine learning model building packages and evaluating their performance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# Plots and graphs packages\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "# packages to save your work in google colab\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZvCupM1WwFM"
      },
      "source": [
        "Here you will read in data from a file called PatData.csv. I (Pleuni Pennings) created that file as a summary of a larger file called \"TADPOLE_D1_D2.csv.\" This has electronic health record data for every patient and this dataset contains all the variables mentioned in our course text plus the other ones and it is measured across several timepoint. **PatData.csv** is a summary, with just one time point per patient and this is the dataset we will be working on in notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XRGmk1gGSxD"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/pleunipennings/CSC508Data/main/PatData.csv\" \n",
        "data = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz_hcncJb-st"
      },
      "source": [
        "# 2) Having a first look at the data \n",
        "\n",
        "As usual we should get into the practice of taking a look at how your data is structured, what is the dimention of our data, which variables are our features and which is a label. For the purpose of this notebook, it is important for us to check what variables we would need to one-hot encode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRVK5s1qfShX"
      },
      "source": [
        "# Checking how big is our data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYesLoZ4c4qO"
      },
      "source": [
        "# Looking at each of the variables column names\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsHfuXTljnVx"
      },
      "source": [
        "# Checking a couple of rows of our data to see what each column data contains\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEzOrudDhkuz"
      },
      "source": [
        "##Task 1: looking at the data\n",
        "\n",
        "a. Each row has data for one patient. How many patients are there in the dataset? \n",
        "\n",
        "b. We are looking at a dataset with just one time point per patient. Why do you \n",
        "think it is useful for an Alzheimer's study to have multiple time points per patient? \n",
        "\n",
        "c. Which of the columns do you think would be important for predicting who has Alzheimer's disease? Pick 2 and explain your choice. \n",
        "\n",
        "d. Which of the columns do you think are not important? Pick 2 and explain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90IJNt2PXWWI"
      },
      "source": [
        "# 3) Data Cleaning: dealing with missing data\n",
        "Now we have the data, but it is messy, with some missing data. Let's see what columns contain missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lkr6NQfsE_o"
      },
      "source": [
        "# This provides counts of missing values for each column\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6T-IO15XqZE"
      },
      "source": [
        "OK, so first of all, let's just focus on patients that have a diagnosis in DX, since this is our target variable or label. \n",
        "Using the dropna() function from pandas https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLeJ5f90Xwx8"
      },
      "source": [
        "# this drops all columns that have missing values in the DX column\n",
        "data = data.dropna(subset=['DX']) \n",
        "# Here we will check again all missing values\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOPsrvIuZGRU"
      },
      "source": [
        "Check how much data we have left after deleting all the rows without DX information. This information should be given to you by looking at the first element of the shape tuple. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RzpUUBCZCK7"
      },
      "source": [
        "# Checking remaining data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU5VmDnOaMLk"
      },
      "source": [
        "And let's look at what diagnoses, column \"DX\" we have, As you can see below We have several diagnoses for the degree of cognitive impairment that range from: **Cognitively Normal** to **Dementia (aka. Alzheimer's Disease)**:\n",
        "\n",
        "|Diagnosis|Meaning|\n",
        "|---|---|\n",
        "|NL|Cognitively normal|\n",
        "|NL to MCI|Person in between Normal and Mild Cognitive Impairment|\n",
        "|MCI|Mild Cognitive Impairment|\n",
        "|MCI to Dementia|Person in between MCI and Alzheimer's Disease|\n",
        "|Dementia|Person that has Alzheimer's Disease|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6qb4QTyZJpy"
      },
      "source": [
        "# get value counts for all diagnoses\n",
        "data['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StG9tM_maXGz"
      },
      "source": [
        "**NOTE:** We should have only 5 diagnosis in total, yet our count shows 8 total diagnosis! This is because real data comes in really messy. Notice for example that we have **NL to Dementia**, this would be the whole spectrum! so clearly this could be the result of an error during data input. \n",
        "\n",
        "Therefore, I would like to take out all the in between diagnosis and keep only the main ones: NL, MCI and Dementia. This way we can work with a simpler classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OscZux15dAQC"
      },
      "source": [
        "# creates an index with the exeptions we have stipulated\n",
        "index_to_drop = data[ (data['DX'] != \"MCI\") & (data['DX'] != \"NL\") & (data['DX'] != \"Dementia\")].index\n",
        "# drops all data based on our index  \n",
        "data = data.drop(index_to_drop)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnRMbj4kaQf1"
      },
      "source": [
        "# this should be the new classification scheme\n",
        "data['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYU1L9MqdhQN"
      },
      "source": [
        "What's the status of missing data now in the other columns? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-l2ah0Idk8E"
      },
      "source": [
        "# checking missing data again\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JyebQ3giFXS"
      },
      "source": [
        "Because we still have a lot of rows of data, we can go ahead and drop all the remaining columns with missing data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvJTitOriNcE"
      },
      "source": [
        "# remove all rows that contain missing data\n",
        "data = data.dropna()\n",
        "# Checking our final dataframe\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lftHwibPic31"
      },
      "source": [
        "OK, so we have 1479 patients with complete data now. We will then check the total number of patients per diagnosis. This is important later on when we train our data with our ML models because we want the number for each diagnosis to be roughly similar, that is as close as possible to a **Balanced** Dataset. When this is not the case, it can present problems in terms of trusting our accuracy results blindly. More about this in **Module 6** and **Module 7**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTYajUznfMjy"
      },
      "source": [
        "# Checking the total number of patients per diagnosis\n",
        "data['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see they are not perfectly equal for all diagnosis but they are close enought that we can proceed."
      ],
      "metadata": {
        "id": "V3rvO6ygIWTf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FblK089oiP8J"
      },
      "source": [
        "# Task 2: describing what we did with missing data\n",
        "\n",
        "In the previous lines of code, we threw out many patients because we didn't have the info we wanted for them. Write a short paragraph where you explain to a potential reader what the number of patients is in the original dataset, which patients we removed for what reason and how many were left for the analysis. Feel free to change the order of operations. For example, I removed first the patients with no diagnosis and later the patients with any missing data. If you do the latter first, you don't have to specifically remove patients with no diagnosis anymore. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy6JCsIWum9N"
      },
      "source": [
        "# 4) Data wrangling in preparation for Model training\n",
        "\n",
        "## - Ensuring feature columns are correct\n",
        "In this section we will be splitting the data into label (the DX columns) and features (All other columns) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3jA9hebBZW5"
      },
      "source": [
        "# Split the data in labels and features\n",
        "labels = data[\"DX\"]\n",
        "features = data.drop(columns=['DX'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgJdu5DruzCY"
      },
      "source": [
        "What do the features look like again? Make sure it doesn't include any columns that will not help in our prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking features again\n",
        "features.head()"
      ],
      "metadata": {
        "id": "20gYiz6dKIvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBrn7Q9M_XBa"
      },
      "source": [
        "Let's remove PTID (patient ID). We don't need it, since it would not help us predict diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY0HR_jy_V2J"
      },
      "source": [
        "# Dropping patient ID column\n",
        "features = features.drop(columns=['PTID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - One Hot Encoding for categorical variables\n",
        "Recall from the text that we are unable to work with data that is words in ML models directly, so we need to recode them into numbers. There are languages and algorythms that seem to be able to deal with words, but they are still turning them into numbers under the hood!\n",
        " "
      ],
      "metadata": {
        "id": "Gtmncu5vL3ZT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZorUXLSw1kR"
      },
      "source": [
        "# checking the variable types for each column\n",
        "features.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8AoQMe_iK7"
      },
      "source": [
        "As we can see in the output above. There are a few features that are not numerical. We need to make sure that **gender**, **ethnicity** , **race** and **marital status** are coded as numbers. To do this we will need to **one-hot-encode** them. Once we are done doing our one hot encoding, we should see each of the levels of our categorical variables as a recoded column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27z-O_k04UP7"
      },
      "source": [
        "# getting a list of categorical variables we need to recode\n",
        "features_to_encode = list(features.select_dtypes(include = ['object']).columns) \n",
        "features_to_encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R206IQYd6RpA"
      },
      "source": [
        "# using a for loop to one-hot encode each of the categorical variables\n",
        "for f in features_to_encode: \n",
        "  print(\"Parent Categorical Variable: \",f)\n",
        "  z = pd.get_dummies(features[f], prefix=f) #get_dummies is the pandas function for one-hot-encoding\n",
        "  features = features.join(z) #append new columns\n",
        "  features = features.drop(columns=[f]) # remove original not recoded column\n",
        "\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Categorical data\n",
        "What type of Categorical variables are we dealing with in our current dataset? Are they ordered or unordered? how many levels does each category have?"
      ],
      "metadata": {
        "id": "R9x46KixOLXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer for question 3**"
      ],
      "metadata": {
        "id": "YdD16aswPXDS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUdDTLcffKvr"
      },
      "source": [
        "Now that we cleaned up the data, it's good to **save the data frame** we now have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdm1_vIWfJfb"
      },
      "source": [
        "# code to save our current dataframe as a csv file\n",
        "features.to_csv('PatData_cleaned_one_hot_encoded.csv', index=False)\n",
        "# code to download our csv file into your own computer\n",
        "files.download('PatData_cleaned_one_hot_encoded.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest with one-Hot Encoded Data\n",
        "\n",
        "Similar to previous modules we can go ahead and prepare our data for training a Random Forest."
      ],
      "metadata": {
        "id": "Tba51GsH2eQF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPd3df_Li5uV"
      },
      "source": [
        "# Separating data into training and testing\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42) # 70% training and 30% test\n",
        "\n",
        "# Creating Random Forest object\n",
        "rf = RandomForestClassifier(n_estimators = 100, max_features = \"auto\", bootstrap = True, random_state = 42)\n",
        "\n",
        "# Training your Random Forest\n",
        "rf.fit(features_train, labels_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "labels_pred = rf.predict(features_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a peak at how our model has performed at least for the first 10 patients of our testing data."
      ],
      "metadata": {
        "id": "b9Knrh2g3ql4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f1Ner7bjRT8"
      },
      "source": [
        "# Look at the predicted values. \n",
        "print(labels_pred[:10])\n",
        "# And the real values. \n",
        "print(labels_test.to_numpy(dtype=object)[:10])\n",
        "#See how many correct predictions there are among the first 10 patients. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the confusin matrix for our Decision tree results\n",
        "print(metrics.confusion_matrix(labels_test, labels_pred))\n",
        "plt2 = metrics.ConfusionMatrixDisplay.from_estimator(rf, features_test, labels_test)\n",
        "plt.grid(False)"
      ],
      "metadata": {
        "id": "ykoJPf864IJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike our other Confusion Matrices, this time we have a 3 by 3 matrix. So this time what we want is for the main diagonal to show us the bigger numbers. What do you think? What diagnosis got the best results?"
      ],
      "metadata": {
        "id": "y7zURXZM7Lnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 4: Write what you noticed about the confusion matrix here!"
      ],
      "metadata": {
        "id": "F_EsIe_68bPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OLbZ1-m9Yg6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) Feature importance\n",
        "\n",
        "Just like in the heart disease notebook, we will now look at the feature importance for the random forest model. \n",
        "\n",
        "As a reminder: Visualizing your results is always an important part of any data science project. Now that we have a random forest based on 1000 random trees, we cannot easily visualize all the trees at once like we did for the decision tree, because it would be an overwhelming set of diagrams. But we can visualize the feature importance. I've seen this kind of plot in published articles. I like it because it helps us understand which features are most important for making predictions.\n",
        "\n",
        "Feature importance is a measurement of how each feature decreases the amount of impurity (Gini index) in a node, weighted by the probability of reaching that node. The higher the value the more important the feature. This is usually calculated for each tree in the random forest and then averaged over the total number of trees. The graph below shows these averages."
      ],
      "metadata": {
        "id": "K7S78Iii8hE_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdsCNF6N1k4Z"
      },
      "source": [
        "# calculating feature importance\n",
        "importance = rf.feature_importances_\n",
        "\n",
        "# summarize feature importance\n",
        "names = features.columns.to_numpy(dtype=object)\n",
        "\n",
        "# Creating a dataframe\n",
        "importanceDF = pd.DataFrame({'names':names, 'importance':importance})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O255gMjX2BE8"
      },
      "source": [
        "#Sort the dataframe based on importance\n",
        "importanceDF = importanceDF.sort_values(by=['importance'])\n",
        "\n",
        "# Plotting feature importance\n",
        "importanceDF.plot.barh(x='names', y='importance', figsize = (7,7), title = \"Feature importance\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il8kv9uZDcda"
      },
      "source": [
        "## Task 4: \n",
        "\n",
        "Look at the pandas documentation pages and make at least two changes to the plot (e.g. colors, width, size, legend etc) \n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.barh.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yprdyqX_kA5B"
      },
      "source": [
        "## Task 5: on Accuracy\n",
        "\n",
        "In addition to a confusion matrix, it is also good to have an accuracy score. \n",
        "\n",
        "a. From sklearn.metrics import the function accuracy_score. \n",
        "\n",
        "b. Use the accuracy_score function to calculate the accuracy of the RF model. \n",
        "\n",
        "c. Once you have stored the accuracy in a variable called accuracy, \n",
        "you can run print(\"Accuracy: %.2f%%\" % (accuracy * 100)). Alternatively, you can use the \"round\" function to round off the accuracy to the desired number of decimals. \n",
        "\n",
        "d. See what happens if you change the 2f into 1f or 3f. \n",
        "\n",
        "e. What level of accuracy is useful for doctors and patients do you think?\n",
        "\n",
        "f. Look at the feature importance bar plot. Compare with your predictions from earlier in the notebook. Were you right or wrong in your predictions? Explain. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI1wRB5dgphZ"
      },
      "source": [
        "# Calculate the accuracy using your code!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer written question 5 here!**"
      ],
      "metadata": {
        "id": "u5RJbEl8-id2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF2TBpMg6c36"
      },
      "source": [
        "# Conclusion of this part\n",
        "What we can see is that the volumes of the parts of the brain are most important, starting with the hippocampus. Gender, race and ethnicity seem least important out of the features we looked at. For race and ethnicity this may be because the data almost entirely consists of non-hispanic whites. "
      ]
    }
  ]
}